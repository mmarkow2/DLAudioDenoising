\documentclass[UTF8]{article}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\usepackage{bbold}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx} 
\usepackage{subfigure}
\usepackage{float}
\usepackage[colorlinks]{hyperref}
\title{Final Project}
\author{Matthew Markowitz, Lifu Xiao}
\date{\today}

\begin{document}
\maketitle


\section{Introduction}
The database is a set of noisy recordings\cite{valentini2017noisy}, which have poor quality for further usage.  So it make sense  to improve them. In order to remove the noise, we propose to use online dictionary learning\cite{mairal2010online}.


There were many challenges involved with this problem. One problem involved the large number of samples that were needed to begin the dictionary updates. Unfortunately, we can't update the dictionary until every atom has been used at least once. To overcome this, we refused to update the dictionary until all the zero elements were filled. However, this is also not ideal because this requires substantially more training data as the window size for our audio increased. Experimentally, this can be overcome by adding a random small constant before we start (such as 0.000000001), however, this is little to no theoretical backing for this, so we did not take this route.


For the problem that we are trying to solve, the state of the art involves using a technique called Spectral Subtraction\cite{upadhyay2015speech}. The general idea in this technique involves estimating the spectrum of background noise from pauses in speech. however there are some limitations. It assume that the background noise in uncorrelated with the speech and there are pauses in the audio that can be used to estimate the background noise. This may not be true for all audio signals. Also, it assumes that the spectrum of background noise is constant which it may not be.


Convolutional neural networks also performance well in audio denoising. An end-to-end learning method based on wavenet\cite{rethage2018wavenet} that learns in a supervised fashion via minimizing a regression loss and it reduces the time-complexity. Another widely used method is deep recurrent neural networks\cite{valentini2016speech} which have good performance on extracting acoustic features from noisy data. To be more specific, Long short-term memory networks are used to avoid the vanishing gradient problem. It use a parallel database of noisy and clean acoustics parameters as input and output. It produce a good result but it have a high computation cost. So we try to apply 


\section{Problem Statement}
We used a python library known as librosa\cite{mcfee2015librosa} to import our audio data. The audio recordings found in our test set had a sampling rate of 22050. This meant that every second of audio held approximately 22,000 numbers to represent it. For this reason, down sampling became necessary. Although some sacrifice in audio quality was necessary, we were able to reduce the sampling rate to 5,000, which made our calculations more feasible. The 5,000 points per second was still computationally intensive, but we found that we could break each second into X millisecond windows to ease computation further without sacrificing much quality.


\section{Algorithm}

\subsection{Data Preparation}

Initializing the $\bm{A}_0 \in \mathbb{R}^{k \times k} \text{ and } \bm{B}_0 \in \mathbb{R}^{m \times k} \text{ as } \vec{\bm{0}} $\\
$k$ is atoms number and $m$ is the dictionary size.

\subsection{Sparse Coding}
When each $x_t$ come, using LARS\cite{scikit-learn} to calculate
\[
	\alpha_t \triangleq \mathop{\arg\min}\limits_{\alpha\in R^k} \frac{1}{2} \| \bm{x}_t - \bm{D}_{t-1} \alpha  \|^2_2 + \lambda \|\alpha\|_1
\]
where $\bm{x} \in \mathbb{R}^{m}, \bm{D} \in \mathbb{R}^{m \times k} \text{ and } t \leq T \text{(maximum number of iterations)}$
\\
Then updating $\bm{A}, \bm{B}$ with mini-batch by
\[\bm{A}_t \leftarrow \bm{A}_{t-1} + \frac{1}{\eta}\sum\limits_{i=1}^{\eta}\alpha_{t,i} \alpha_{t,i}^T\]
\[\bm{B}_t \leftarrow \bm{B}_{t-1} + \frac{1}{\eta}\sum\limits_{i=1}^{\eta}\bm{x}_{t,i} \alpha_{t,i}^T\]

\subsection{Dictionary Update}
\[\bm{D}_t \triangleq \mathop{\arg\min}\limits_{\bm{D} \in C} \frac{1}{t} \sum{ \frac{1}{2} \| \bm{x}_i - \bm{D} \alpha_i  \|^2_2 + \lambda \|\alpha\|_1}
\]
Where $C \triangleq {\bm{D} \in \mathbb{R}^{m \times k} \text{ s.t. } \forall j = 1, ..., k, \bm{d}_j^T\bm{d}_j \leq 1}$ to ensure the convex.
\\
Using block-coordinate descent to update dictionary
Extracting columns of $\bm{A} \text{ and } \bm{B}$
\[\bm{A} = [\bm{a}_1, ..., \bm{a}_k] \in \mathbb{R}^{k \times k}\]
\[\bm{B} = [\bm{b}_1, ..., \bm{b}_k] \in \mathbb{R}^{k \times k}\]
for each column from $j = 1 \Rightarrow k$
\[\bm{u_j} \leftarrow \frac{1}{A[j, j]}(\bm{b}_j - \bm{Da}_j) + \bm{d}_j\]
\[\bm{d}_j \leftarrow \frac{1}{\max(\|\bm{u}_j\|_2, 1)}\bm{u}_j\]
return $\bm{D}$ for next iteration


\section{Experiments}

To visualize our result, we picked a 6 seconds segment of the output audio.
The Original, Downsampled figures and the Clean data which is used for a baseline are as follows.


\begin{figure}[H]
    \centering
    \subfigure[Original]{
        \label{Fig.sub1.1}
        \includegraphics[width=0.8\textwidth]{image/Original.png}}
    \subfigure[Downsampled]{
        \label{Fig.sub1.2}
        \includegraphics[width=0.8\textwidth]{image/Downsampled.png}}
    \subfigure[CleanData]{
        \label{Fig.sub1.3}
        \includegraphics[width=0.9\textwidth]{image/Clean.png}}
    \caption{Original , Downsampled and the Clean data}
\end{figure}


We set $\lambda = 0.025$ and test different window sizes for the denoising program. The results are presented in Figure 2


\begin{figure}[H]
    \centering
    \subfigure[Windows size = 50]{
        \label{Fig.sub2.1}
        \includegraphics[width=0.4\textwidth]{image/50.png}}
    \subfigure[Windows size = 100]{
        \label{Fig.sub2.2}
        \includegraphics[width=0.4\textwidth]{image/100.png}}
    \subfigure[Windows size = 150]{
        \label{Fig.sub2.3}
        \includegraphics[width=0.4\textwidth]{image/150.png}}
    \subfigure[Windows size = 200]{
        \label{Fig.sub2.4}
        \includegraphics[width=0.4\textwidth]{image/200.png}}
    \subfigure[Windows size = 250]{
        \label{Fig.sub2.5}
        \includegraphics[width=0.4\textwidth]{image/250.png}}
    \subfigure[Windows size = 300]{
        \label{Fig.sub2.6}
        \includegraphics[width=0.4\textwidth]{image/300.png}}
    \caption{Denoising result under different window sizes}
\end{figure}


The smaller window sizes give the better result. Now we choose $k = 500$ and $m = 50$ for the online dictionary learning step and the result generated by different $\lambda$ are presented in Figure 3.


\begin{figure}[H]
    \centering
    \subfigure[$\lambda = 0.005$]{
        \label{Fig.sub3.1}
        \includegraphics[width=0.9\textwidth]{image/0_0001.png}}
    \subfigure[$\lambda = 0.025$]{
        \label{Fig.sub3.2}
        \includegraphics[width=0.9\textwidth]{image/0_0005.png}}
    \subfigure[$\lambda = 0.035$]{
        \label{Fig.sub3.3}
        \includegraphics[width=0.9\textwidth]{image/0_0007.png}}
    \subfigure[$\lambda = 0.05$]{
        \label{Fig.sub3.4}
        \includegraphics[width=0.9\textwidth]{image/0_001.png}}
    \caption{Denoising result under different $\lambda$}
\end{figure}

We note that $\lambda = 0.025$ have the best performance among three other  $\lambda$s.

So we set $\lambda = 0.025$ to test different atoms. We set atoms from $ 1 \times$ dictionary size to $ 15 \times$ dictionary size.

\begin{figure}[H]
    \centering
    \subfigure[1$\times$atoms]{
        \label{Fig.sub4.1}
        \includegraphics[width=0.4\textwidth]{image/1x.png}}
    \subfigure[2$\times$atoms]{
        \label{Fig.sub4.2}
        \includegraphics[width=0.4\textwidth]{image/2x.png}}
    \subfigure[5$\times$atoms]{
        \label{Fig.sub4.3}
        \includegraphics[width=0.4\textwidth]{image/5x.png}}
    \subfigure[7$\times$atoms]{
        \label{Fig.sub4.4}
        \includegraphics[width=0.4\textwidth]{image/7x.png}}
    \subfigure[10$\times$atoms]{
        \label{Fig.sub4.5}
        \includegraphics[width=0.4\textwidth]{image/10x.png}}
    \subfigure[15$\times$atoms]{
        \label{Fig.sub4.6}
        \includegraphics[width=0.4\textwidth]{image/15x.png}}
    \caption{Denoising result under different atoms}
\end{figure}

After we examine the output, the $15 \times \text{atoms}$ turned to have a better precise. We also test different batch sizes which are illustrated as follows.

\begin{figure}[H]
    \centering
    \subfigure[Batch size 2]{
        \label{Fig.sub5.1}
        \includegraphics[width=0.4\textwidth]{image/2.png}}
    \subfigure[Batch size 5]{
        \label{Fig.sub5.2}
        \includegraphics[width=0.4\textwidth]{image/5.png}}
    \subfigure[Batch size 7]{
        \label{Fig.sub5.3}
        \includegraphics[width=0.4\textwidth]{image/7.png}}
    \subfigure[Batch size 10]{
        \label{Fig.sub5.4}
        \includegraphics[width=0.4\textwidth]{image/10.png}}
    \subfigure[Batch size 13]{
        \label{Fig.sub5.5}
        \includegraphics[width=0.4\textwidth]{image/13.png}}
    \subfigure[Batch size 15]{
        \label{Fig.sub5.6}
        \includegraphics[width=0.4\textwidth]{image/15.png}}
    \caption{Denoising result under different atoms}
\end{figure}

Finally we randomly pick seven atoms and visualize them to show that how do they work.

\begin{figure}[H]
    \centering
    \subfigure[Atom 1]{
        \label{Fig.sub6.1}
        \includegraphics[width=0.4\textwidth]{image/Atom1.png}}
    \subfigure[Atom 2]{
        \label{Fig.sub6.2}
        \includegraphics[width=0.4\textwidth]{image/Atom2.png}}
    \subfigure[Atom 3]{
        \label{Fig.sub6.3}
        \includegraphics[width=0.4\textwidth]{image/Atom3.png}}
    \subfigure[Atom 4]{
        \label{Fig.sub6.4}
        \includegraphics[width=0.4\textwidth]{image/Atom4.png}}
    \subfigure[Atom 5]{
        \label{Fig.sub6.5}
        \includegraphics[width=0.4\textwidth]{image/Atom5.png}}
    \subfigure[Atom 6]{
        \label{Fig.sub6.6}
        \includegraphics[width=0.4\textwidth]{image/Atom6.png}}
    \subfigure[Atom 7]{
        \label{Fig.sub6.7}
        \includegraphics[width=0.4\textwidth]{image/Atom7.png}}
    \caption{Atoms}
\end{figure}

Our experiments have showed that online dictionary learning is very efficient and robust on audio denoising problem. Most of our judgements are based on the output audio files which is not ocular. An audio waveform provides a more direct way to make comparison, however, the difference is not easy to be noticed. Finding another judgement model can be very interesting.

%  Reference
\bibliographystyle{unsrt}
\bibliography{reference}
\nocite{*}

\end{document}