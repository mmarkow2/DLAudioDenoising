\documentclass[UTF8]{article}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\usepackage{bbold}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx} 
\usepackage{subfigure}
\usepackage{float}
\usepackage[colorlinks]{hyperref}
\title{Final Project}
\author{Matthew Markowitz, Lifu Xiao}
\date{\today}

\begin{document}
\maketitle


\section{Introduction}
The database is a set of noisy recordings, which have poor quality for further usage. So it make sense  to improve them. In order to remove the noise, we propose to use online dictionary learning.



\section{Problem Statement}
We used a python library known as librosa to import our audio\cite{valentini2016speech}. The audio recordings found in our test set had a sampling rate of 22050. This meant that every second of audio held approximately 22,000 numbers to represent it. For this reason, down sampling became necessary. Although some sacrifice in audio quality was necessary, we were able to reduce the sampling rate to 5,000, which made our calculations more feasible. The 5,000 points per second was still computationally intensive, but we found that we could break each second into X millisecond windows to ease computation further without sacrificing much quality. We found that a window size of 50 points or 50/5,000 = 1/100 second windows worked well for our dataset.



\section{Algorithm}

\subsection{Data Preparation}

Initializing the $\bm{A}_0 \in \mathbb{R}^{k \times k} \text{ and } \bm{B}_0 \in \mathbb{R}^{m \times k} \text{ as } \vec{\bm{0}} $\\
$k$ is atoms number and $m$ is the dictionary size.

\subsection{Sparse Coding}
When each $x_t$ come, using LARS\cite{scikit-learn} to calculate
\[
	\alpha_t \triangleq \mathop{\arg\min}\limits_{\alpha\in R^k} \frac{1}{2 \cdot k} \| \bm{x}_t - \bm{D}_{t-1} \alpha  \|^2_2 + \lambda \|\alpha\|_1
\]
where $\bm{x} \in \mathbb{R}^{m}, \bm{D} \in \mathbb{R}^{m \times k} \text{ and } t \leq T \text{(maximum number of iterations)}$
\\
Then updating $\bm{A}, \bm{B}$ by
\[\bm{A}_t \leftarrow \bm{A}_{t-1} + \alpha_t \alpha_t^T\]
\[\bm{B}_t \leftarrow \bm{B}_{t-1} + \bm{x}_t \alpha_t^T\]

\subsection{Dictionary Update}
\[\bm{D}_t \triangleq \mathop{\arg\min}\limits_{\bm{D} \in C} \frac{1}{t} \sum{ \frac{1}{2} \| \bm{x}_i - \bm{D} \alpha_i  \|^2_2 + \lambda \|\alpha\|_1}
\]
Where $C \triangleq {\bm{D} \in \mathbb{R}^{m \times k} \text{ s.t. } \forall j = 1, ..., k, \bm{d}_j^T\bm{d}_j \leq 1}$ to ensure the convex.
\\
Using block-coordinate descent to update dictionary
Extracting columns of $\bm{A} \text{ and } \bm{B}$
\[\bm{A} = [\bm{a}_1, ..., \bm{a}_k] \in \mathbb{R}^{k \times k}\]
\[\bm{B} = [\bm{b}_1, ..., \bm{b}_k] \in \mathbb{R}^{k \times k}\]
for each column from $j = 1 \Rightarrow k$
\[\bm{u_j} \leftarrow \frac{1}{A[j, j]}(\bm{b}_j - \bm{Da}_j) + \bm{d}_j\]
\[\bm{d}_j \leftarrow \frac{1}{\max(\|\bm{u}_j\|_2, 1)}\bm{u}_j\]
return $\bm{D}$ for next iteration


\section{Experiments}
The Original and Downsampled figures are as follows.
\begin{figure}[H]
    \centering
    \subfigure[Original]{
        \label{Fig.sub1.1}
        \includegraphics[width=0.9\textwidth]{image/Original.png}}
    \subfigure[Original]{
        \label{Fig.sub1.2}
        \includegraphics[width=0.9\textwidth]{image/Downsampled.png}}
    \caption{Original and Downsampled}
\end{figure}

We choose $k = 500 \text{ and } m = 50$ for the online dictionary learning step and the result generated by different $\lambda$ are presented in Figure 2.

\begin{figure}[H]
    \centering
    \subfigure[$\lambda = 0.0001$]{
        \label{Fig.sub2.1}
        \includegraphics[width=0.9\textwidth]{image/0_0001.png}}
    \subfigure[$\lambda = 0.0005$]{
        \label{Fig.sub2.2}
        \includegraphics[width=0.9\textwidth]{image/0_0005.png}}
    \subfigure[$\lambda = 0.0007$]{
        \label{Fig.sub2.3}
        \includegraphics[width=0.9\textwidth]{image/0_0007.png}}
    \subfigure[$\lambda = 0.001$]{
        \label{Fig.sub2.4}
        \includegraphics[width=0.9\textwidth]{image/0_001.png}}
    \caption{Output}
\end{figure}

%  Reference
\bibliographystyle{unsrt}
\bibliography{reference}
\nocite{*}

\end{document}